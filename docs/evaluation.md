# Evaluation System

## Overview

Every story generated by Kids Story Agent receives a **comprehensive quality evaluation** across 5 dimensions. The evaluation uses LLM-based structured outputs to provide consistent, interpretable scores that help reviewers make informed decisions.

## Evaluation Dimensions

### 1. Moral Score (1-10)

**Definition**: Does the story teach positive values and ethical lessons?

**Scoring Criteria**:
- **9-10**: Strong moral message (kindness, honesty, courage, empathy, sharing)
- **7-8**: Good moral message with clear positive values
- **5-6**: Neutral or weak moral message
- **3-4**: Ambiguous or potentially negative values
- **1-2**: Negative or harmful values

**Examples**:
- **High Score**: Story about sharing toys, helping friends, being honest
- **Low Score**: Story promoting selfishness, dishonesty, or harmful behavior

**Weight**: 25% of overall score

### 2. Theme Appropriateness (1-10)

**Definition**: Is the theme suitable, engaging, and developmentally appropriate for the target age group?

**Scoring Criteria**:
- **9-10**: Perfectly matched theme, highly engaging for age group
- **7-8**: Good theme fit, age-appropriate
- **5-6**: Acceptable but not ideal for age group
- **3-4**: Questionable theme for age group
- **1-2**: Inappropriate theme for age group

**Age-Specific Considerations**:
- **3-5 years**: Simple themes (friendship, sharing, basic emotions)
- **6-8 years**: More complex themes (adventure, problem-solving, empathy)
- **9-12 years**: Advanced themes (justice, responsibility, growth)

**Weight**: 20% of overall score

### 3. Emotional Positivity (1-10)

**Definition**: Does the story evoke warmth, joy, hope, and comfort rather than fear, anxiety, or sadness?

**Scoring Criteria**:
- **9-10**: Highly positive, uplifting, joyful
- **7-8**: Generally positive with minor challenges
- **5-6**: Mixed emotions, some negative elements
- **3-4**: More negative than positive emotions
- **1-2**: Predominantly negative, distressing emotions

**Examples**:
- **High Score**: Story with happy ending, warm relationships, positive resolution
- **Low Score**: Story with sad ending, fear, abandonment, or distress

**Weight**: 25% of overall score

### 4. Age Appropriateness (1-10)

**Definition**: Is the vocabulary, sentence structure, and content complexity right for the target age?

**Scoring Criteria**:
- **9-10**: Perfect vocabulary and complexity for age
- **7-8**: Good match, minor adjustments needed
- **5-6**: Acceptable but could be better tuned
- **3-4**: Too simple or too complex for age
- **1-2**: Completely inappropriate complexity level

**Factors Considered**:
- **Vocabulary**: Word choice, sentence length
- **Concepts**: Abstract vs. concrete thinking
- **Narrative Structure**: Simple vs. complex plots
- **Character Development**: Depth appropriate for age

**Weight**: 20% of overall score

### 5. Educational Value (1-10)

**Definition**: Does the child learn something valuable from the story?

**Scoring Criteria**:
- **9-10**: Strong educational value (social skills, knowledge, problem-solving)
- **7-8**: Good educational takeaways
- **5-6**: Some educational value
- **3-4**: Minimal educational value
- **1-2**: No educational value or negative learning

**Types of Learning**:
- **Social Skills**: Empathy, cooperation, communication
- **Knowledge**: Facts, concepts, world understanding
- **Problem-Solving**: Critical thinking, creativity
- **Emotional Intelligence**: Understanding emotions, self-regulation

**Weight**: 10% of overall score

## Scoring Methodology

### LLM-Based Evaluation

The evaluation uses structured LLM outputs with a specialized system prompt:

```python
class StoryEvalOutput(BaseModel):
    moral_score: float = Field(ge=1, le=10)
    theme_appropriateness: float = Field(ge=1, le=10)
    emotional_positivity: float = Field(ge=1, le=10)
    age_appropriateness: float = Field(ge=1, le=10)
    educational_value: float = Field(ge=1, le=10)
    evaluation_summary: str
```

### System Prompt

```
You are a children's content quality evaluator for a kids story platform.
Score the following story on each dimension from 1 to 10.
Target age group: {age_group}.

Scoring rubric:
- moral_score: Does the story teach positive values? (kindness, honesty, courage, sharing, empathy)
- theme_appropriateness: Is the theme suitable, engaging, and developmentally appropriate for the age group?
- emotional_positivity: Does the story evoke warmth, joy, hope, and comfort? (not fear, anxiety, sadness)
- age_appropriateness: Is the vocabulary, sentence structure, and content complexity right for the age?
- educational_value: Does the child learn something valuable? (social skills, knowledge, problem-solving, empathy)

Be strict — this content goes directly to children. Provide an honest evaluation_summary with specific examples from the story.
```

### Weighted Overall Score

The overall score is computed as a weighted average:

```python
EVAL_WEIGHTS = {
    "moral": 0.25,        # 25%
    "theme": 0.20,       # 20%
    "emotional": 0.25,    # 25%
    "age": 0.20,         # 20%
    "edu": 0.10,         # 10%
}

overall_score = (
    moral_score * 0.25 +
    theme_appropriateness * 0.20 +
    emotional_positivity * 0.25 +
    age_appropriateness * 0.20 +
    educational_value * 0.10
)
```

**Example**:
- Moral: 8.0 × 0.25 = 2.0
- Theme: 7.5 × 0.20 = 1.5
- Emotional: 9.0 × 0.25 = 2.25
- Age: 8.5 × 0.20 = 1.7
- Educational: 6.0 × 0.10 = 0.6
- **Overall**: 8.05/10

## Evaluation Output

### Structured Response

```json
{
  "evaluation_scores": {
    "moral_score": 8.0,
    "theme_appropriateness": 7.5,
    "emotional_positivity": 9.0,
    "age_appropriateness": 8.5,
    "educational_value": 6.0,
    "overall_score": 8.05,
    "evaluation_summary": "This story effectively teaches the value of sharing through a heartwarming narrative about two friends. The vocabulary is appropriate for ages 6-8, and the positive resolution reinforces empathy. However, the educational value could be enhanced with more explicit problem-solving elements."
  }
}
```

### Evaluation Summary

The `evaluation_summary` provides:
- **Brief narrative assessment** of the story's quality
- **Specific examples** from the story supporting the scores
- **Strengths and weaknesses** identified
- **Recommendations** for improvement (if applicable)

## Integration with Workflow

### Evaluation Node

The `story_evaluator` node runs in parallel with guardrail checks:

```python
def story_evaluator_node(state: StoryState) -> dict:
    llm = get_llm()
    structured_llm = llm.with_structured_output(StoryEvalOutput)
    
    output = structured_llm.invoke(messages)
    
    return {
        "evaluation_scores": {
            "moral_score": output.moral_score,
            "theme_appropriateness": output.theme_appropriateness,
            "emotional_positivity": output.emotional_positivity,
            "age_appropriateness": output.age_appropriateness,
            "educational_value": output.educational_value,
            "overall_score": compute_weighted_score(output),
            "evaluation_summary": output.evaluation_summary,
        },
    }
```

### Parallel Execution

- **Story Evaluator**: Runs concurrently with guardrail checks
- **Fan-out**: One evaluator per story (no parallelization needed)
- **Fan-in**: Results aggregated in state for review package

## Using Evaluation Scores

### Human Review

Evaluation scores are included in the review package:

```json
{
  "job_id": "...",
  "story_title": "...",
  "story_text": "...",
  "evaluation_scores": {
    "overall_score": 8.05,
    "moral_score": 8.0,
    "theme_appropriateness": 7.5,
    "emotional_positivity": 9.0,
    "age_appropriateness": 8.5,
    "educational_value": 6.0,
    "evaluation_summary": "..."
  },
  "guardrail_violations": [],
  "image_urls": [...],
  "video_urls": [...]
}
```

### Review Decision Support

Reviewers can use scores to:
1. **Quick Assessment**: Overall score provides quick quality indicator
2. **Detailed Analysis**: Individual dimension scores highlight strengths/weaknesses
3. **Summary Review**: Evaluation summary explains reasoning
4. **Comparison**: Compare scores across multiple stories

### Quality Thresholds

**Recommended Thresholds**:
- **Overall Score ≥ 8.0**: High quality, likely approve
- **Overall Score 6.0-7.9**: Good quality, review carefully
- **Overall Score < 6.0**: Lower quality, may need revision

**Note**: Scores are advisory only. Human reviewers make final decisions.

## Performance

### Latency

- **Evaluation Time**: ~2-3 seconds (LLM call)
- **Token Usage**: ~1500-2500 tokens per evaluation
- **Cost**: ~$0.01-0.02 per evaluation (GPT-4)

### Accuracy

- **Consistency**: Structured outputs ensure consistent format
- **Reliability**: LLM provides nuanced assessment
- **Bias**: May have inherent biases (monitor and adjust)

## Customization

### Adjusting Weights

Modify `EVAL_WEIGHTS` in `story_evaluator.py`:

```python
EVAL_WEIGHTS = {
    "moral": 0.30,        # Increase moral weight
    "theme": 0.20,
    "emotional": 0.30,    # Increase emotional weight
    "age": 0.15,         # Decrease age weight
    "edu": 0.05,         # Decrease educational weight
}
```

### Custom Scoring Rubrics

Modify `EVAL_SYSTEM_PROMPT` to adjust scoring criteria:

```python
EVAL_SYSTEM_PROMPT = """You are a children's content quality evaluator.
[Custom rubric here]
"""
```

### Age-Specific Prompts

Different prompts for different age groups:

```python
AGE_SPECIFIC_PROMPTS = {
    "3-5": "Focus on simple vocabulary, basic emotions, clear moral lessons...",
    "6-8": "Focus on adventure themes, problem-solving, empathy...",
    "9-12": "Focus on complex themes, character development, critical thinking...",
}
```

## Monitoring

### Key Metrics

- **Average Overall Score**: Track quality trends over time
- **Score Distribution**: Histogram of scores (identify quality clusters)
- **Dimension Breakdown**: Average scores per dimension
- **Low Score Rate**: Percentage of stories scoring < 6.0

### Logging

All evaluations are logged:

```
INFO: Job abc123: Evaluation complete — overall score 8.05/10 (moral=8.0, theme=7.5, emotional=9.0, age=8.5, edu=6.0)
```

## Best Practices

### Interpreting Scores

1. **Overall Score**: Quick quality indicator, but review individual dimensions
2. **Dimension Scores**: Identify specific strengths/weaknesses
3. **Summary**: Read evaluation summary for context
4. **Context**: Consider story genre, age group, intended use

### Using Scores in Review

1. **High Scores (≥8.0)**: Likely approve, but still review for edge cases
2. **Medium Scores (6.0-7.9)**: Review carefully, check specific dimensions
3. **Low Scores (<6.0)**: Review thoroughly, may need revision or rejection

### Continuous Improvement

1. **Monitor Trends**: Track average scores over time
2. **Feedback Loop**: Adjust weights based on review decisions
3. **A/B Testing**: Test different prompts/weights
4. **Human Calibration**: Compare LLM scores to human ratings

## Future Enhancements

### Potential Improvements

1. **Multi-LLM Evaluation**: Average scores from multiple LLMs
2. **Fine-Tuned Models**: Train custom evaluation models
3. **Learning from Reviews**: Adjust weights based on approval rates
4. **Real-Time Feedback**: Update evaluation criteria without code changes
5. **Comparative Evaluation**: Compare stories to benchmark set
